# Cymbal Assist â€” Real-time Call Center Agent Assist

Real-time call-center agent-assist dashboard that transcribes customer speech live,
retrieves context from a Vertex AI RAG corpus, and streams RAG-grounded Gemini responses â€”
all in a WhatsApp-style chat UI the agent can read while on the call.

## Architecture

```
Browser Mic â”€â”€â–º AudioWorklet (20ms PCM-16 chunks)
                    â”‚
                WebSocket (binary)
                    â”‚
               FastAPI Server (Cloud Run)
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                       â–¼
  Streaming STT             Gemini 2.5 Flash
  (Chirp 3, V2 API)        + RAG Tool (google.genai SDK)
  interim + final           streaming chunks
  transcripts               via send_message_stream
        â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
             WebSocket (JSON)
                    â”‚
           Agent Dashboard
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ğŸ¤ Customer         â”‚ ğŸ¤– AI Assistant
        â”‚ (live transcript)   â”‚ (streamed, RAG-grounded)
        â”‚                     â”‚ â± latency badge
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline** (no TTS â€” agent reads responses on screen):

```
Audio (PCM-16, 16kHz) â†’ Streaming STT (Chirp 3) â†’ Final transcript
    â†’ Gemini 2.5 Flash (with RAG tool, streaming) â†’ Agent dashboard
```

## Key Features

- **Streaming STT** â€” Chirp 3 model via Cloud Speech V2 with interim results for real-time display
- **Auto language detection** â€” Chirp 3 `auto` mode or select from 11 Indian languages
- **RAG-grounded responses** â€” Vertex AI RAG Engine corpus as a native Gemini tool (retrieval)
- **Streaming LLM** â€” `send_message_stream` with thread + asyncio.Queue pattern (non-blocking)
- **Chat memory** â€” persistent `ChatSession` keeps full conversation context
- **AudioWorklet** â€” dedicated audio thread with 20ms PCM chunks, downsampled to 16kHz
- **Latency tracking** â€” per-response TTFC (Time To First Chunk) and total response time in UI
- **WhatsApp-style UI** â€” customer bubbles (left), AI bubbles (right) with streaming text

## Tech Stack

| Layer | Technology |
|---|---|
| **Frontend** | Vanilla JS, AudioWorklet, WebSocket |
| **Server** | FastAPI + Uvicorn, Python 3.12 |
| **STT** | Google Cloud Speech-to-Text V2 (Chirp 3), `us` multi-region |
| **LLM** | Gemini 2.5 Flash via `google-genai` SDK (Vertex AI backend) |
| **RAG** | Vertex AI RAG Engine (corpus as Gemini retrieval tool) |
| **Deploy** | Cloud Build â†’ Cloud Run (`us-central1`) |

## Prerequisites

- Python 3.12+
- Google Cloud SDK (`gcloud auth application-default login`)
- GCP project with:
  - Cloud Speech-to-Text V2 API enabled
  - Vertex AI API enabled
  - Vertex AI RAG Engine corpus provisioned
  - Cloud Run API enabled

## Quick Start

### 1. Clone & install

```bash
git clone https://github.com/Ashishkamble004/cymbalassist.git
cd cymbalassist
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure

Create a `.env` file (or set environment variables):

```bash
GCP_PROJECT_ID=your-project-id
GCP_LOCATION=us-central1
RAG_CORPUS_RESOURCE_NAME=projects/your-project/locations/us-central1/ragCorpora/YOUR_CORPUS_ID
STT_MODEL=chirp_3
STT_LOCATION=us
STT_LANGUAGE=auto
LLM_MODEL=gemini-2.5-flash
```

### 3. Authenticate to GCP

```bash
gcloud auth application-default login
```

### 4. Run locally

```bash
python run.py
```

Open **http://localhost:8000** â€” click **Connect & Start**, allow mic access, and start speaking.

### 5. Deploy to Cloud Run

```bash
gcloud builds submit --config=cloudbuild.yaml --region=us-central1
```

## Project Structure

```
cymbalassist/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py                # Core pipeline: Streaming STT â†’ RAG â†’ LLM (streaming)
â”‚   â”œâ”€â”€ config.py               # Settings from env vars / .env
â”‚   â”œâ”€â”€ server.py               # FastAPI server with WebSocket endpoint
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ audio-processor.js  # AudioWorklet: 20ms PCM-16 capture at 16kHz
â”‚       â””â”€â”€ index.html          # Agent dashboard (WhatsApp-style chat UI)
â”œâ”€â”€ cloudbuild.yaml             # Cloud Build â†’ Cloud Run pipeline
â”œâ”€â”€ Dockerfile                  # Python 3.12-slim container
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.py                      # Entry point (uvicorn)
â””â”€â”€ README.md
```

## Key Components

| Component | File | Description |
|---|---|---|
| `StreamingSTTManager` | `agent.py` | Manages Chirp 3 streaming via background thread + audio queue |
| `run_agent` | `agent.py` | WebSocket handler: creates genai client, RAG tool, chat session |
| `process_with_rag_llm` | `agent.py` | Streams Gemini response via thread + asyncio.Queue (non-blocking) |
| `PCMProcessor` | `audio-processor.js` | AudioWorklet: accumulates 20ms of audio, downsamples, sends PCM-16 |
| Dashboard | `index.html` | WhatsApp-style chat with streaming bubbles + latency badges |

## WebSocket Protocol

### Client â†’ Server (binary)
Raw PCM-16 audio chunks (16kHz, mono, little-endian).

### Server â†’ Client (JSON)

| `type` | `role` | Description |
|---|---|---|
| `transcription` | `user` | Interim (`is_final: false`) or final (`is_final: true`) transcript |
| `stream_start` | `assistant` | LLM stream beginning â€” create empty bubble |
| `stream_chunk` | `assistant` | LLM text chunk â€” append to bubble |
| `stream_end` | `assistant` | LLM stream complete â€” finalize bubble |
| `error` | â€” | Error message |

## Configuration

| Variable | Default | Description |
|---|---|---|
| `GCP_PROJECT_ID` | `general-ak` | Google Cloud project |
| `GCP_LOCATION` | `us-central1` | Vertex AI / Cloud Run region |
| `RAG_CORPUS_RESOURCE_NAME` | â€” | Full resource name of RAG corpus |
| `STT_MODEL` | `chirp_3` | Speech-to-Text model |
| `STT_LOCATION` | `us` | STT multi-region endpoint |
| `STT_LANGUAGE` | `auto` | Language code(s) â€” `auto` for auto-detect |
| `LLM_MODEL` | `gemini-2.5-flash` | Gemini model |
| `HOST` | `0.0.0.0` | Server bind address |
| `PORT` | `8000` | Server port (Cloud Run overrides to 8080) |
